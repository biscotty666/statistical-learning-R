---
title: "Linear Model Selection and Regularization"
format: 
  html:
    toc: true
    keep-md: true
  gfm: default
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(paged.print = FALSE)
options(digits = 3)
```

# Subset selection methods

## Best Subset Selection

`leaps::regsubsets()`

```{r}
library(ISLR2)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```

```{r}
nrow(Hitters[!complete.cases(Hitters), ])
```

```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
library(leaps)
```

```{r}
regfit_full <- regsubsets(Salary ~ ., Hitters)
summary(regfit_full)
```

```{r}
reg_summary <- regsubsets(Salary ~ ., Hitters, nvmax = 19) |>
  summary()
```

```{r}
names(reg_summary)
```

```{r}
reg_summary$rsq
```

```{r}
par(mfrow = c(2, 2))
plot(reg_summary$rss,
  type = "l",
  xlab = "Number of Variables", ylab = "RSS"
)
max_adjr2 <- which.max(reg_summary$adjr2)
plot(reg_summary$adjr2,
  type = "l",
  xlab = "Number of Variables", ylab = "Adjusted RSq"
)
points(max_adjr2, reg_summary$adjr2[max_adjr2], col = "red", cex = 2, pch = 20)
min_cp <- which.min(reg_summary$cp)
plot(reg_summary$cp,
  type = "l",
  xlab = "Number of Variables", ylab = "Cp"
)
points(min_cp, reg_summary$cp[min_cp], col = "red", cex = 2, pch = 20)
min_bic <- which.min(reg_summary$bic)
plot(reg_summary$bic,
  type = "l",
  xlab = "Number of Variables", ylab = "BIC"
)
points(min_bic, reg_summary$bic[min_bic], col = "red", cex = 2, pch = 20)
```

```{r}
purrr::map(
  c("r2", "adjr2", "Cp", "bic"),
  \(x) plot(regfit_full, scale = x)
)
```

6 variables were best for BIC

```{r}
coef(regfit_full, 6)
```

## Forward and Backward Stepwise selection

```{r}
regfit_fwd <- regsubsets(Salary ~ .,
  data = Hitters,
  nvmax = 19, method = "forward"
)
regfit_bwd <- regsubsets(Salary ~ .,
  data = Hitters,
  nvmax = 19, method = "backward"
)
```

```{r}
coef(regfit_full, 7)
coef(regfit_fwd, 7)
coef(regfit_bwd, 7)
```

## Choosing with Validation-Set and Cross-Validation

```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters),
  replace = TRUE
)
test <- (!train)
```

```{r}
regfit_best <- regsubsets(Salary ~ .,
  data = Hitters[train, ],
  nvmax = 19
)
test_mat <- model.matrix(Salary ~ ., data = Hitters[test, ])

test_mat[1:6, 1:6]
```

```{r}
val_errors <- rep(NA, 19)
for (i in 1:19) {
  coefi <- coef(regfit_best, id = i)
  pred <- test_mat[, names(coefi)] %*% coefi
  val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val_errors
```

```{r}
val_errors <- rep(NA, 19)
val_error_fn <- function(i) {
  coefi <- coef(regfit_best, id = i)
  pred <- test_mat[, names(coefi)] %*% coefi
  val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
purrr::map_dbl(1:19, val_error_fn)
```

The best model has seven variables.

```{r}
which.min(val_errors)
coef(regfit_best, 7)
```

```{r}
predict_regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}
```

```{r}
regfit_best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(regfit_best, 7)
```

Cross-validation with k=10 training sets

```{r}
k <- 10
n <- nrow(Hitters)
```

```{r}
folds <- sample(rep(1:k, length = n))
cv_errors <- matrix(NA, k, 19,
  dimnames = list(NULL, paste(1:19))
)
for (j in 1:k) {
  # best_fit <- regsubsets(Salary ~ .,
  #                      data = Hitters[folds != j, ],
  #                     nvmax = 19)
  best_fit <- regsubsets(Salary ~ .,
    data = Hitters[folds != j, ],
    nvmax = 19
  )
  for (i in 1:19) {
    pred <- predict_regsubsets(best_fit, Hitters[folds == j, ], id = i)
    cv_errors[j, i] <-
      mean((Hitters$Salary[folds == j] - pred)^2)
  }
}
```

```{r}
cv_errors
```

```{r}
mean_cv_errors <- apply(cv_errors, 2, mean)
mean_cv_errors
```

```{r}
par(mfrow = c(1, 1))
plot(mean_cv_errors, type = "b")
```

```{r}
reg_best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(reg_best, 10)
```

# Ridge Regression and Lasso

```{r}
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
```

## Ridge Regression

```{r}
#| message: false
library(glmnet)
```

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge_mod <- glmnet(x, y, alpha = 0, lambda = grid)
```
`glmnet()` standardizes by default. 

When $\lambda = 11498$
```{r}
ridge_mod$lambda[50]
coef(ridge_mod)[, 50]
sqrt(sum(coef(ridge_mod)[-1, 50]^2))
```

When $\lambda=705$
```{r}
ridge_mod$lambda[60]
coef(ridge_mod)[, 60]
sqrt(sum(coef(ridge_mod)[-1, 60]^2))
```

> Use `predict()` for a new $\lambda$ value

For $\lambda=50$
```{r}
predict(ridge_mod, s = 50, type = "coefficients")[1:20, ]
```

> Test/Train split

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y_test <- y[test]
```

```{r}
ridge_mod <- glmnet(x[train, ], y[train],
  alpha = 0,
  lambda = grid, thresh = 1e-12
)
ridge_pred <- predict(ridge_mod, s = 4, newx = x[test, ])
mean((ridge_pred - y_test)^2)
```

Compare ridge regression $\lambda=4$ to least square regression.

```{r}
ridge_pred <- predict(ridge_mod,
  s = 0, newx = x[test, ],
  exact = T, x = x[train, ], y = y[train]
)
mean((ridge_pred - y_test)^2)
lm(y ~ x, subset = train)
predict(ridge_mod,
  s = 0, exact = T, type = "coefficients",
  x = x[train, ], y = y[train]
)[1:20, ]
```

> Using cross-validation to determine $\lambda$

```{r}
set.seed(1)
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv_out)
```

```{r}
best_lamda <- cv_out$lambda.min
best_lamda
```

MSE 

```{r}
ridge_pred <- predict(ridge_mod,
  s = best_lamda,
  newx = x[test, ]
)
mean((ridge_pred - y_test)^2)
```

> Refit on full set with best_lambda

```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = best_lamda)[1:20, ]
```

All variables are included, since RR does not do variable selection.

## Lasso

```{r}
lasso_mod <- glmnet(x[train, ], y[train],
  alpha = 1, lambda = grid
)
plot(lasso_mod)
```

> Cross validate cand calculate test error

```{r}
set.seed(1)
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv_out)
```

```{r}
best_lamda <- cv_out$lambda.min
lasso_pred <- predict(lasso_mod,
  s = best_lamda,
  newx = x[test, ]
)
mean((lasso_pred - y_test)^2)
```

```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso_coef <- predict(out, type = "coefficients", s = best_lamda)[1:20, ]
lasso_coef
```

# PCR and PLS Regression

## Principle Components Regression

```{r}
#| message: false
library(pls)
```

```{r}
set.seed(2)
pcr_fit <- pcr(Salary ~ .,
  data = Hitters, scale = TRUE, # scale standardizes variables
  validation = "CV"
)
```

```{r}
summary(pcr_fit)
```

X can be understood as percentage of explained variance.

```{r}
validationplot(pcr_fit, val.type = "MSEP")
```

```{r}
set.seed(1)
pcr_fit <- pcr(Salary ~ .,
  data = Hitters, subset = train,
  scale = TRUE, validation = "CV"
)
validationplot(pcr_fit, val.type = "MSEP")
```

The lowest validation error is $M = 5$.

```{r}
pcr_pred <- predict(pcr_fit, x[test, ], ncomp = 5)
mean((pcr_pred - y_test)^2)
```

The answer is close to lasso and rr, but difficult to interpret.

```{r}
pcr_fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)
summary(pcr_fit)
```

## Partial Least Squares

```{r}
set.seed(1)
pls_fit <- plsr(Salary ~ .,
  data = Hitters, subset = train,
  scale = TRUE, validation = "CV"
)
summary(pls_fit)
```

```{r}
validationplot(pls_fit, val.type = "MSEP")
```

```{r}
pls_pred <- predict(pls_fit, x[test, ], ncomp = 1)
mean((pls_pred - y_test)^2)
```

Slightly higher than rr and lasso.

With the full dataset

```{r}
pls_fit <- plsr(Salary ~ .,
  data = Hitters,
  scale = TRUE, ncomp = 1
)
summary(pls_fit)
```






