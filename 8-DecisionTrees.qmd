---
title: "Decision Trees"
format: 
  html:
    toc: true
    keep-md: true
  gfm: default
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(paged.print = FALSE)
options(digits = 4)
```

# Fitting Classification Trees

```{r}
library(tree)
library(ISLR2)
```

```{r}
high <- factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
carseats <- data.frame(Carseats, high)
```

Fit classification tree with High as target and all variables except Sales

```{r}
tree_carseats <- tree(high ~ . - Sales, carseats)
summary(tree_carseats)
```

```{r}
plot(tree_carseats)
text(tree_carseats, pretty = 0)
```

```{r}
summary(tree_carseats)
```

```{r}
class(tree_carseats)
```

```{r}
#| eval: false
tree_carseats
```

Estimate training error using test/train split

```{r}
set.seed(2)
train <- sample(1:nrow(carseats), 200)
carseats_test <- carseats[-train, ]
high_test <- high[-train]
tree_carseats <- tree(high ~ . - Sales, carseats, subset = train)
tree_pred <- predict(tree_carseats, carseats_test,
  type = "class"
)
table(tree_pred, high_test)
(104 + 50) / 200
```

> Consider pruning

Use `cv.tree()` to determine tree complexity. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. 

```{r}
set.seed(7)
cv_carseats <- cv.tree(tree_carseats,
  FUN = prune.misclass
)
names(cv_carseats)
```

```{r}
cv_carseats
```

Size is the number of terminal nodes on each tree with error rate and cost complexity parameter (k). The tree with 9 terminal nodes has 74 CV errors.

```{r}
par(mfrow = c(1, 2))
plot(cv_carseats$size, cv_carseats$dev, type = "b")
plot(cv_carseats$k, cv_carseats$dev, type = "b")
```

Prune to the 9-node tree.

```{r}
prune_carseats <- prune.misclass(tree_carseats, best = 9)
```

```{r}
plot(prune_carseats)
text(prune_carseats)
```

```{r}
tree_pred <- predict(prune_carseats, carseats_test,
  type = "class"
)
table(tree_pred, high_test)
(97 + 58) / 200
```

Improved accuracy and interpretability.

# Fitting Regression Trees

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree_boston <- tree(medv ~ ., Boston, subset = train)
summary(tree_boston)
```

The deviance is the sum of squared errors.

```{r}
plot(tree_boston)
text(tree_boston, pretty = 0, digits = 3)
```

rm is the number of rooms, lstat is socioeconomic status.

It is worth noting that we could have fit a much bigger tree, by pass-
ing control = tree.control(nobs = length(train), mindev = 0) into the
tree() function.

> Check if pruning will improve performance

```{r}
cv_boston <- cv.tree(tree_boston)
plot(cv_boston$size, cv_boston$dev, type = "b")
```

```{r}
prune_boston <- prune.tree(tree_boston, best = 6)
plot(prune_boston)
text(prune_boston, pretty = 0, digits = 3)
```

```{r}
yhat <- predict(tree_boston, newdata = Boston[-train, ])
boston_test <- Boston[-train, "medv"]
plot(yhat, boston_test)
abline(0, 1)
```

```{r}
mean((yhat - boston_test)^2)
```


# Bagging and Random Forests

Bagging is a random forest where $m = p$.

```{r}
#| message: false
library(randomForest)
```

```{r}
set.seed(1)
bag_boston <-
  randomForest(medv ~ .,
    data = Boston, subset = train,
    mtry = 12, importance = TRUE
  )
bag_boston
```

`mtry` indicates the number of predictors to be considered. In this case, all 12.

```{r}
yhat_bag <- predict(bag_boston, newdata = Boston[-train, ])
plot(yhat_bag, boston_test)
abline(0, 1)
```

```{r}
mean((yhat_bag - boston_test)^2)
```

A thirty percent improvement over the pruned single tree.

Set number of trees

```{r}
bag_boston <-
  randomForest(medv ~ .,
    data = Boston, subset = train,
    mtry = 12, ntree = 25
  )
yhat_bag <- predict(bag_boston, newdata = Boston[-train, ])
mean((yhat_bag - boston_test)^2)
```

Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and âˆšp variables when building a random forest of classification trees. 

```{r}
set.seed(1)
rf_boston <- randomForest(medv ~ .,
  data = Boston, subset = train,
  mtry = 6, importance = TRUE
)
yhat_rf <- predict(rf_boston, newdata = Boston[-train, ])
mean((yhat_rf - boston_test)^2)
```

> Check the importance of each variable

```{r}
importance(rf_boston)
```

```{r}
varImpPlot(rf_boston)
```

Confirming community wealth and house size as the most important factors.

# Boosting

We run gbm() with the option distribution = "gaussian" since this is a regression problem; if it were a binary classification problem, we would use distribution = "bernoulli". `interaction.depth` limits the depth of each tree.

```{r}
#| message: false
library(gbm)
```

```{r}
set.seed(1)
boost_boston <- gbm(medv ~ .,
  data = Boston[train, ],
  distribution = "gaussian", n.trees = 5000,
  interaction.depth = 4
)
summary(boost_boston)
```

> Partial dependence plots

```{r}
plot(boost_boston, i = "rm")
plot(boost_boston, i = "lstat")
```

```{r}
yhat_boost <- predict(boost_boston,
  newdata = Boston[-train, ],
  n.trees = 5000
)
mean((yhat_boost - boston_test)^2)
```

Superior to random forests and bagging.

Vary the shrinkage parameter. Default is $\lambda=0.001$.

```{r}
boost_boston <- gbm(medv ~ .,
  data = Boston[train, ],
  distribution = "gaussian", n.trees = 5000,
  interaction.depth = 4, shrinkage = 0.2
)
yhat_boost <- predict(boost_boston,
  newdata = Boston[-train, ],
  n.trees = 5000
)
mean((yhat_boost - boston_test)^2)
```

# Bayesian Additive Regression Trees

`gbart()` for quantitative, `lbart()` and `pbart()` for binary.

```{r}
#| message: false
library(BART)
```

```{r}
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bart_fit <- gbart(xtrain, ytrain, x.test = xtest)
```

Compute test error

```{r}
yhat_bart <- bart_fit$yhat.test.mean
mean((ytest - yhat_bart)^2)
```

Even lower.

How many times did each variable occur in the trees?

```{r}
ord <- order(bart_fit$varcount.mean, decreasing = T)
bart_fit$varcount.mean[ord]
```


















